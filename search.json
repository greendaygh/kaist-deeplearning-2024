[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KAIST Deep Learning",
    "section": "",
    "text": "1 KAIST EB502 programming",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "KAIST Deep Learning",
    "section": "1.1 Welcome",
    "text": "1.1 Welcome\n\n2024.11 카이스트, 공학생물학대학원 프로그래밍 강의 노트\nHaseong Kim (at KRIBB)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#환경",
    "href": "index.html#환경",
    "title": "KAIST Deep Learning",
    "section": "1.2 환경",
    "text": "1.2 환경\n\n실습 환경은 colab을 활용하며 파일 저장 등은 구글 드라이브를 활용함",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "Learn python for biological data analysis with chatGPT\ncolab의 default working directory에 개인의 google drive 연결\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n2.0.1 Setting Up Google Colab\n\n2.0.1.1 Access Google Colab\n\nGo to Google Colab in your web browser.\nSign in with your Google account.\n\n\n\n2.0.1.2 Create a New Notebook\n\nClick on File -&gt; New Notebook to create a new notebook.\n\n\n\n2.0.1.3 Install Required Libraries\nGoogle Colab comes with many libraries pre-installed, but you might need to install some additional ones, such as biopython and scikit-bio. You can do this using the !pip install command directly in a cell.\n\n!pip install biopython scikit-bio matplotlib\n\n\n!pip install scikit-bio\n\n\n\n\n2.0.2 Import Libraries and Verify Installation\nIn a new code cell, import the libraries to ensure they are installed correctly.\n\n# Importing necessary libraries\nimport Bio\nimport skbio\n\nprint(\"Biopython version:\", Bio.__version__)\nprint(\"scikit-bio version:\", skbio.__version__)\n\nBiopython version: 1.84\nscikit-bio version: 0.6.2\n\n\n\n\n2.0.3 Upload Files to Colab\n\nCreate 2024-kaist-lecture folder\nipynb file open with colab\nDownload ganbank files from ncbi and upload the files\ncurrent directory\n\n\n!pwd\n\n/home/haseong/lecture/kaist-deeplearning-2024\n\n\n\n현재 작업 디렉토리를 위 생성한 디렉토리로 변경\n\n\nimport os\nos.chdir('drive/MyDrive/2024-kaist-lecture')\n\n\n!pwd\n\n/content/drive/MyDrive/2024-kaist-lecture\n\n\n\n분석을 위한 genbank 등의 파일을 ncbi에서 다운로드 후 위 폴더에 복사\n또는 아래 코드를 이용해서 현재 작업 디렉토리에 업로드\n\n\nfrom google.colab import files\n\nuploaded = files.upload()\n\n# Listing the uploaded files\nfor filename in uploaded.keys():\n    print(filename)\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving nn.png to nn.png\nnn.png\n\n\n\n\n\nimage.png\n\n\n\n\n2.0.4 Example: Reading a GenBank File and Analyzing Sequences\n\nHere’s an example workflow that reads a GenBank file, performs sequence analysis with Biopython, and aligns sequences with scikit-bio:\n위에서 다운로드 받은 genbank 파일을 egfp.gb 이름으로 변경 후 업로드\n아래 예시 코드는 단백질 서열을 읽고 비교하는 코드임\n\n\nfrom Bio import SeqIO\nfrom Bio.Seq import Seq\nfrom skbio import Protein\nfrom skbio.alignment import global_pairwise_align_protein\n\n# Assuming the uploaded GenBank file is named 'example.gb'\ngenbank_file = 'data/egfp.gb'\n\n# Read a protein sequence from a GenBank file\nrecord = SeqIO.read(genbank_file, \"genbank\")\nprint(len(record.seq))\n\nprotein_seq = record.seq.translate()\nprint(protein_seq)\n\n# Perform alignment with another protein sequence\nseq1 = Protein(str(protein_seq))\nseq2 = Protein(\"MKVLYNLKDG\")\n\nalignment, score, start_end_positions = global_pairwise_align_protein(seq1, seq2)\n\nprint(\"Alignment:\\n\", alignment)\nprint(\"Score:\", score)\nprint(\"Start-End Positions:\", start_end_positions)\n\nModuleNotFoundError: No module named 'Bio'\n\n\n\n\n3 pytorch\nPyTorch is an open-source deep learning framework developed by Facebook’s AI Research lab (FAIR). It is widely used in research and industry due to its dynamic computation graph and ease of use.\nPyTorch Ecosystem Overview:\ntorch: The core library for tensor operations and automatic differentiation. torch.nn: A sub-library used to build and train neural network models. torch.optim: Tools for optimization algorithms (e.g., SGD, Adam). torchvision: Provides datasets, pre-trained models, and image transformations.\n\n\n4 Linear regression\n\n4.0.1 Simple linear regression\n\\(y_i = b_0 + b_1 x_i + e_i\\)\n\nimport numpy as np\n\nnp.random.seed(42)\nX = np.array(range(1, 10, 2))\nY = X*2 + np.random.normal(0, 1, len(X))\n\nX_b = np.c_[np.ones(len(X)), X]\ntheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(Y)\nprint(theta_best)\n\n[0.4091132  2.00997796]\n\n\n\\(y = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b\\)\nTensors\nTensors are the primary data structures in PyTorch, analogous to NumPy arrays but with added capabilities such as the ability to run on GPUs for faster computation.\n\nimport torch\n\ntensor = torch.tensor([[1,2], [3,4]], dtype=torch.float32)\nprint(tensor)\n\ntensor([[1., 2.],\n        [3., 4.]])\n\n\n\n\n\n5 Neural networks\nNeural networks are computational models inspired by the human brain, used to recognize patterns and perform tasks like classification, regression, and more complex functions in various fields including computer vision, natural language processing, and robotics.\n\n5.0.1 Key Characteristics of Neural Networks:\n\nArchitecture:\n\nNeurons (Nodes): Basic units of a neural network that take inputs, perform a computation, and pass outputs to the next layer.\nLayers:\n\nInput Layer: The starting point where data is fed into the network.\nHidden Layers: Intermediate layers between the input and output that perform complex transformations on the data. More hidden layers and nodes lead to deeper and more powerful models.\nOutput Layer: The final layer that produces the result (e.g., classification label, numerical prediction).\n\n\nWeights and Biases:\n\nWeights: Parameters that determine the strength of the connection between neurons. They are adjusted during training to optimize the network’s performance.\nBiases: Additional parameters added to the input of a neuron to allow more flexibility in model learning.\n\nActivation Functions:\n\nActivation functions introduce non-linearity to the model, enabling it to learn complex patterns.\nCommon activation functions include:\n\nReLU (Rectified Linear Unit): ( f(x) = (0, x) )\nSigmoid: ( f(x) = )\nTanh: ( f(x) = )\n\n\nForward Pass:\n\nThe process of passing input data through the network, layer by layer, to produce an output.\nEach neuron computes a weighted sum of its inputs, adds a bias, and passes the result through an activation function.\n\nLoss Function:\n\nA measure of how well the neural network’s predictions match the actual data. Common loss functions include Mean Squared Error (MSE) for regression and Cross-Entropy Loss for classification.\n\nBackpropagation and Training:\n\nBackpropagation: The process by which the network learns by adjusting weights and biases based on the loss. It uses gradients computed by the chain rule to update these parameters through gradient descent or other optimization algorithms.\nOptimization Algorithm: Techniques like Stochastic Gradient Descent (SGD) or Adam adjust the model parameters to minimize the loss.\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimg = mpimg.imread('images/nn.png')\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.0.2 How a Neural Network Works:\nImagine a simple neural network with one input layer, one hidden layer, and one output layer. Here’s how it functions: 1. Input data is passed to the input layer. 2. The input is transformed and propagated through the hidden layer(s), where each neuron applies weights, biases, and an activation function. 3. The result reaches the output layer, which produces the final prediction. 4. The loss function calculates the error between the predicted output and the true value. 5. Backpropagation is used to adjust the weights and biases to reduce the loss in future iterations.\n\n\n5.0.3 Simple Example:\nA neural network to classify handwritten digits (like in the MNIST dataset) might: - Input Layer: Accept an image of a digit (28x28 pixels). - Hidden Layers: Extract features and learn patterns in the image. - Output Layer: Produce probabilities for each digit (0-9), indicating the most likely classification.\n\n\n5.0.4 Applications of Neural Networks:\n\nImage recognition (e.g., identifying objects in pictures)\nSpeech recognition (e.g., voice-to-text)\nNatural language processing (e.g., language translation)\nMedical diagnosis (e.g., detecting diseases from scans)\n\nNeural networks have evolved into more complex structures such as Convolutional Neural Networks (CNNs) for image tasks and Recurrent Neural Networks (RNNs) for sequential data, enhancing their ability to solve specialized problems.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  }
]